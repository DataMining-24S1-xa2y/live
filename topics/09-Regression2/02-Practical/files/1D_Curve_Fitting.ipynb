{
  "cells": [
    {
      "cell_type": "raw",
      "id": "f154e77a",
      "metadata": {},
      "source": [
        "---\n",
        "pairNB CA1spec.py\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97c3ec42",
      "metadata": {},
      "source": [
        "# Lab - Univariate regression with Polynomial Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0e0a80f",
      "metadata": {
        "lines_to_next_cell": 1,
        "tags": [
          "imports"
        ]
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
        "\n",
        "import seaborn as sns\n",
        "import pingouin as pg\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import RidgeCV\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from scipy.special import struve\n",
        "\n",
        "from scipy.stats import norm, uniform\n",
        "\n",
        "# The following can be used to combine pdfs into 1\n",
        "#from matplotlib.backends.backend_pdf import PdfPages\n",
        "#\n",
        "# See https://stackoverflow.com/a/11329151\n",
        "#pp = PdfPages('output/combined.pdf')\n",
        "#pp.savefig(fig) # add figure using its fig handle\n",
        "\n",
        "colours = sns.color_palette('colorblind')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0cddc4c",
      "metadata": {},
      "source": [
        "## Generate Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d25b549",
      "metadata": {
        "lines_to_next_cell": 1,
        "tags": [
          "defineSignalF1"
        ]
      },
      "outputs": [],
      "source": [
        "def f1(x):\n",
        "  return struve(0,10*x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b51eafe3",
      "metadata": {
        "tags": [
          "defineX"
        ]
      },
      "outputs": [],
      "source": [
        "X = uniform.rvs(loc=0, scale=1, size=50, random_state=42)\n",
        "X.sort()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "163ce671",
      "metadata": {
        "tags": [
          "plotTrueSignal"
        ]
      },
      "outputs": [],
      "source": [
        "true_signal = f1(X)\n",
        "plt.plot(X, true_signal, 'k.-')\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"f1\")\n",
        "plt.title(f\"True signal f1 ({len(X)} points, x distributed according to uniform distribution)\")\n",
        "plt.xlim(0,1)\n",
        "plt.ylim(-1,3)\n",
        "#plt.savefig('output/trueSignal.pdf')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97a54635",
      "metadata": {},
      "source": [
        "#### Derive Noise and display plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d99a53f3",
      "metadata": {
        "tags": [
          "generateNoise"
        ]
      },
      "outputs": [],
      "source": [
        "noise = norm.rvs(loc=0, scale=1, size=len(X), random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60e6eca9",
      "metadata": {
        "tags": [
          "plotNoiseHist"
        ]
      },
      "outputs": [],
      "source": [
        "sns.displot(noise, kde=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0381148e",
      "metadata": {
        "tags": [
          "plotNoiseQQ"
        ]
      },
      "outputs": [],
      "source": [
        "pg.qqplot(noise)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fc1e11c",
      "metadata": {},
      "source": [
        "#### Add the generated noise and plot the noisy signal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52382b89",
      "metadata": {
        "tags": [
          "addNoise"
        ]
      },
      "outputs": [],
      "source": [
        "y = true_signal + 0.1 * noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91c5be47",
      "metadata": {
        "tags": [
          "plotNoisySignal"
        ]
      },
      "outputs": [],
      "source": [
        "plt.plot(X, y, \"k.\");\n",
        "plt.xlim(0,1)\n",
        "plt.ylim(-1,3)\n",
        "plt.title(f\"Noisy signal f1 ({len(X)} points, x distributed according to uniform distribution)\")\n",
        "plt.xlabel(\"x\")\n",
        "#plt.savefig('output/noisySignal.pdf')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e35f449",
      "metadata": {},
      "source": [
        "## Split Data into train and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97ba51b5",
      "metadata": {
        "tags": [
          "trainTestSplit"
        ]
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.40, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9ec4d68",
      "metadata": {
        "lines_to_next_cell": 1,
        "tags": [
          "plotTrainTest"
        ]
      },
      "outputs": [],
      "source": [
        "plt.scatter(X_train, y_train, color=colours[0], marker=\"+\", label='train');\n",
        "plt.scatter(X_test, y_test, color=colours[1], marker=\".\", label='test');\n",
        "plt.title(f\"Actual Noisy signal ({len(y_train)} train, {len(y_test)} test data)\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.xlim(0,1)\n",
        "plt.ylim(-1,3)\n",
        "plt.legend(frameon=True)\n",
        "#plt.savefig('output/trainTest.pdf')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27c33f1e",
      "metadata": {},
      "source": [
        "## Construct Features for a Polynomial Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5cb165e",
      "metadata": {
        "lines_to_next_cell": 1,
        "tags": [
          "prepPolyFeatures"
        ]
      },
      "outputs": [],
      "source": [
        "def preparePolyFeatures(degree, X_train, y_train):\n",
        "  poly = PolynomialFeatures(degree)\n",
        "  # fit = train = determine optimal parameters using the train dataset only\n",
        "  poly.fit(X_train.reshape(-1, 1))\n",
        "  # transform = apply transformation to both train and test\n",
        "  X_train_poly = poly.transform(X_train.reshape(-1, 1))\n",
        "  X_test_poly = poly.transform(X_test.reshape(-1, 1))\n",
        "  return poly, X_train_poly, X_test_poly"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef898309",
      "metadata": {},
      "source": [
        "## For linearly-spaced points, prepare the predicted polynomial learned from the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf798ff2",
      "metadata": {
        "lines_to_next_cell": 1,
        "tags": [
          "prepPolyPoints"
        ]
      },
      "outputs": [],
      "source": [
        "def preparePolyPoints(poly, model):\n",
        "  # generate extra ordered points for smooth curve\n",
        "  X_plot = np.linspace(0, 1, 100)\n",
        "  X_plot_poly = poly.transform(X_plot.reshape(-1, 1))\n",
        "  y_plot_pred = model.predict(X_plot_poly)\n",
        "  return X_plot, X_plot_poly, y_plot_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ff81829",
      "metadata": {},
      "source": [
        "## For the polynomial features derived from the training data, use OLS to predict the beta parameters and compute their range"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0e46310",
      "metadata": {
        "lines_to_next_cell": 1,
        "tags": [
          "defOlsFit"
        ]
      },
      "outputs": [],
      "source": [
        "def olsFit(X_train_poly, y_train):\n",
        "  # Fit a Linear Regression model to the polynomial features\n",
        "  model = LinearRegression()\n",
        "  model.fit(X_train_poly, y_train)\n",
        "  betaPoly = model.coef_\n",
        "  betaPolyRange = np.max(betaPoly)-np.min(betaPoly)\n",
        "  return model, betaPoly, betaPolyRange"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecea5191",
      "metadata": {},
      "source": [
        "## For the polynomial features derived from the training data, use Regularised Linear Regression fit with cross-validation over the candidate lambdas, to predict the beta parameters and compute their range"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39f09e4e",
      "metadata": {
        "lines_to_next_cell": 1,
        "tags": [
          "defRegularisedLRfit"
        ]
      },
      "outputs": [],
      "source": [
        "def regularisedLRFit(X_train_poly, y_train, lambdas):\n",
        "  # Fit a regularised Linear Regression model to the polynomial features\n",
        "  model = RidgeCV(alphas=lambdas, scoring = 'neg_mean_squared_error', store_cv_results=True)\n",
        "  model.fit(X_train_poly, y_train)\n",
        "  betaPoly = model.coef_\n",
        "  cvScores = model.cv_results_\n",
        "  bestLambda = model.alpha_\n",
        "  betaPolyRange = np.max(betaPoly)-np.min(betaPoly)\n",
        "  return model, betaPoly, betaPolyRange, cvScores, bestLambda"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "295452b9",
      "metadata": {},
      "source": [
        "## Compute the mean square error between the actual and predicted targets, using the model learned from the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0735d6ee",
      "metadata": {
        "lines_to_next_cell": 1,
        "tags": [
          "defMSE"
        ]
      },
      "outputs": [],
      "source": [
        "def prepMSE(model, X_poly, y):\n",
        "  # Compute the mean_squared_error between the actual and predicted targets\n",
        "  y_pred = model.predict(X_poly)\n",
        "  mseScore = mean_squared_error(y, y_pred) \n",
        "  return mseScore"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f474a433",
      "metadata": {},
      "source": [
        "## Plot training and test sets, with the predicted target from the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efab79c2",
      "metadata": {
        "lines_to_next_cell": 1,
        "tags": [
          "prepPlotFit"
        ]
      },
      "outputs": [],
      "source": [
        "def prepPlotFit(rType, X_train, y_train, X_test, y_test, X_plot, y_plot_pred, degree):\n",
        "  # prepare the figure showing the fitted polynomial against the training and test data\n",
        "  # global fig\n",
        "  # neded so that return is not needed from this function - otherwise run into bug in notebook_exporter\n",
        "  fig = plt.figure()\n",
        "  plt.scatter(X_train, y_train, color=colours[0], marker=\"+\", label='actual (train)')\n",
        "  plt.scatter(X_test, y_test, color=colours[1], marker=\".\", label='actual (test)')\n",
        "  plt.plot(X_plot, y_plot_pred, color=colours[4], label='model')\n",
        "  plt.title(f\"Plot (degree={degree}) {rType} polynomial fit to Actual ({len(y_train)} training, {len(y_test)} test) points\")\n",
        "  plt.xlabel(\"x\")\n",
        "  plt.legend(frameon=True)\n",
        "  plt.ylim(-1,3)\n",
        "  plt.savefig(f'output/{rType}/trainTest{degree}.pdf')\n",
        "  plt.show()\n",
        "  plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f105f150",
      "metadata": {},
      "source": [
        "## Derive polynomial features for a specified degree, fit the training data, compute MSE for the training and test data, plot the results in context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f2e1635",
      "metadata": {
        "lines_to_next_cell": 1,
        "tags": [
          "defFitAndPlot"
        ]
      },
      "outputs": [],
      "source": [
        "def fitAndPlot(X_train, y_train, Xtest, y_test, degree, rType):\n",
        "  poly, X_train_poly, X_test_poly = preparePolyFeatures(degree, X_train, y_train)\n",
        "  if (rType == 'OLS'):\n",
        "    model, betaPoly, betaPolyRange = olsFit(X_train_poly, y_train)\n",
        "    bestLambda = 0.0\n",
        "  else:\n",
        "    lambdas = np.logspace(-3.0, 4.0, num=16, base=10.0)\n",
        "    model, betaPoly, betaPolyRange, cvLambdaScores, bestLambda = regularisedLRFit(X_train_poly, y_train, lambdas)\n",
        "  # Now use K=5-fold cross-validation to estimate the MSE of the model fit, relative to the validation data (the left-out data) \n",
        "  cv_results = cross_validate(model, X_train_poly, y_train, cv=5,\n",
        "               scoring=('neg_mean_squared_error'),  return_train_score=True)\n",
        "  mseCV = -1*np.mean(cv_results['test_score'])\n",
        "  # For comparison, we can compute the training and test MSE scores\n",
        "  mseTrain = prepMSE(model, X_train_poly, y_train)\n",
        "  mseTest = prepMSE(model, X_test_poly, y_test)\n",
        "  X_plot, X_plot_poly, y_plot_pred = preparePolyPoints(poly, model)\n",
        "  # prepare the figure showing the fitted polynomial against the training and test data\n",
        "  prepPlotFit(rType, X_train, y_train, X_test, y_test, X_plot, y_plot_pred, degree)\n",
        "  return mseCV, mseTrain, mseTest, betaPolyRange, betaPoly, bestLambda"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d373a98b",
      "metadata": {},
      "source": [
        "## Plot the mean square error (MSE) against polynomial degree, for training and test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15597c5c",
      "metadata": {
        "lines_to_next_cell": 1,
        "tags": [
          "defPlotMSEforDegrees"
        ]
      },
      "outputs": [],
      "source": [
        "def plotMSEforDegrees(rType, mseDf):\n",
        "  fig = plt.figure()\n",
        "  plt.scatter(mseDf.index, mseDf['train'], color=colours[0], marker=\"+\", label='train MSE')\n",
        "  #plt.scatter(mseDf.index, mseDf['test'], color=colours[1], marker=\".\", label='test MSE')\n",
        "  plt.scatter(mseDf.index, mseDf['cvMSE'], color=colours[4], marker=\"x\", label='cv MSE')\n",
        "  plt.title(f\"{rType} MSE for train and cross-validation against degree\")\n",
        "  plt.xlabel(\"polynomial degree\")\n",
        "  plt.legend(frameon=True)\n",
        "  plt.savefig(f\"output/{rType}/MSEforDegrees.pdf\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77148cd4",
      "metadata": {},
      "source": [
        "## Plot the range (max-min) of fitted polynomial coefficients against polynomial degree, for training and test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be1978b4",
      "metadata": {
        "lines_to_next_cell": 1,
        "tags": [
          "defPlotCoeffsForDegrees"
        ]
      },
      "outputs": [],
      "source": [
        "def plotCoeffsForDegrees(rType, mseDf):\n",
        "  fig = plt.figure()\n",
        "  plt.plot(mseDf.index, mseDf['betaPolyRange'], \"k.-\", label='range beta')\n",
        "  plt.title(f\"{rType} Range (max-min) beta (polynomial coefficients) against degree\")\n",
        "  plt.xlabel(\"polynomial degree\")\n",
        "  plt.legend(frameon=True)\n",
        "  plt.savefig(f\"output/{rType}/coeffsForDegrees.pdf\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83dedec5",
      "metadata": {},
      "source": [
        "## Plot the regularisation parameters lambda (found using cross validation) against polynomial degree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4ed8d24",
      "metadata": {
        "lines_to_next_cell": 1,
        "tags": [
          "defPlotLambdasForDegrees"
        ]
      },
      "outputs": [],
      "source": [
        "def plotLambdasForDegrees(rType, mseDf):\n",
        "  fig = plt.figure()\n",
        "  plt.plot(mseDf.index, mseDf['bestLambda'], \"k.-\", label='best lambda')\n",
        "  plt.title(f\"{rType} best lambda against degree\")\n",
        "  plt.xlabel(\"polynomial degree\")\n",
        "  plt.legend(frameon=True)\n",
        "  plt.savefig(f\"output/{rType}/lambdasForDegrees.pdf\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "698e5939",
      "metadata": {},
      "source": [
        "## Fit, using rType procedure, a polynomial to training data, plotting the results and returning a summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef1ed180",
      "metadata": {
        "lines_to_next_cell": 1,
        "tags": [
          "defFitPlotAndSave"
        ]
      },
      "outputs": [],
      "source": [
        "def fitPlotAndSave(X_train, y_train, X_test, y_test, maxDegree, rType):\n",
        "  cvMSE = []\n",
        "  trainMSE = []\n",
        "  testMSE = []\n",
        "  #coeffs = []\n",
        "  ranges = []\n",
        "  bestLambdas = []\n",
        "  for degree in range(0,maxDegree+1):\n",
        "    mseCV, mseTrain, mseTest, betaPolyRange, betaPoly, bestLambda = fitAndPlot(X_train, y_train, X_test, y_test, degree, rType)\n",
        "    \n",
        "    cvMSE.append(mseCV)\n",
        "    trainMSE.append(mseTrain)\n",
        "    testMSE.append(mseTest)\n",
        "    ranges.append(betaPolyRange)\n",
        "    #coeffs.append(betaPoly)\n",
        "    bestLambdas.append(bestLambda)\n",
        "  \n",
        "  degrees = list(range(0,maxDegree+1))\n",
        "  rTypes = [rType] * len(degrees) \n",
        "  mseDf = pd.DataFrame({\n",
        "    'rType': rTypes,\n",
        "    'degree': degrees,\n",
        "    'bestLambda': bestLambdas,\n",
        "    'cvMSE': cvMSE,\n",
        "    'train': trainMSE,\n",
        "    'test': testMSE,\n",
        "    'betaPolyRange': ranges\n",
        "    })\n",
        "  plotMSEforDegrees(rType, mseDf)\n",
        "  plotCoeffsForDegrees(rType, mseDf)\n",
        "  plotLambdasForDegrees(rType, mseDf)\n",
        "  return mseDf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caa4e44b",
      "metadata": {},
      "source": [
        "## \"Main program\": predict y using varying degrees of polynomial, with or without regularisation, with diagnostic output including plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3de1320",
      "metadata": {
        "tags": [
          "runRegularisedPlotFits"
        ]
      },
      "outputs": [],
      "source": [
        "# See https://stackoverflow.com/a/76020741 and https://stackoverflow.com/a/67548340\n",
        "data = []\n",
        "for rType in ('OLS', 'regularisedLR'):\n",
        "  data.append(fitPlotAndSave(X_train, y_train, X_test, y_test, maxDegree=12, rType=rType))\n",
        "mseDf = pd.concat(data, axis=0, ignore_index=True)\n",
        "mseDf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0251c132",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "source": [
        "## General comments\n",
        "\n",
        "a) The OLS fit indicates that degree 4-6 offers good performance. Beyond this point, there are signs of overfitting.\n",
        "b) Regularisation behaviour is slightly harder to interpret, but it looks that higher degree plots don't wiggle as much as they would without regularisation. So it looks like, for this case, that degree = 9 might have the lowest MSE.\n",
        "\n",
        "Note the double use of cross-validation\n",
        "1. using `RidgeCV()` to find `bestLambda` for a given polynomial degree, when regularisation is used\n",
        "2. using `cross_validate()` to estimate the uncertainty in the model for a given choice of `degree`, when either OLS or regularisation is used.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
